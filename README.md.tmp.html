<meta charset="UTF-8">
<h1>Social Network Project</h1>

<h2>Progress board</h2>

<p>The progress board can be accessed <a href="https://github.com/SamuelsGitHub/Social_Network_Project/projects/1">here</a>.</p>

<h2>Detailed analysis of paper</h2>

<h3>1. Dataset</h3>

<ul>
<li><code>df_trump.csv</code>: 28.42% users wrote multiple tweets, so 
278188 - 82970 * (1 - 0.2842) = 218798.1 tweets written by 23580 users (avg 9.28 tweets/user which tweet multiple times)</li>
<li><code>df_hillary.csv</code>: 29.88% users wrote multiple tweets, so
346730 - 161194 * (1 - 0.2988) = 233700.8 tweets written by 45811.33 users (avg 5.1 tweets/user which tweet multiple times)</li>
<li><code>df_ferguson.csv</code>: 39.47% users wrote multiple tweets, so
552911 - 181179 * (1 - 0.3947) = 443243.4 tweets written by 71511.35 users (avg 6.2 tweets/user which tweet multiple times)</li>
<li><code>df_samesex.csv</code>: 31.89% users wrote multiple tweets, so
980018 - 389111 * (1 - 0.3189) = 714994.5 tweets written by 124087.5 users (avg 5.76 tweets/user which tweet multiple times)</li>
</ul>

<p><strong>Conclusion:</strong> Dataset not as diverse as it seems, although still somewhat OK</p>

<h3>2. SentiStrength</h3>

<p>From <a href="https://github.com/SamuelsGitHub/Social_Network_Project/blob/main/paper_sentistrength.pdf">SentiStrength paper (Thelwall, Buckley et al., 2010)</a>:</p>

<ul>
<li>"Overall, it seems that SentiStrength is not good at identifying negative emotion"; "results for negative sentiment are not significant"</li>
<li>Accuracy for positive emotion 60.6%, for negative emotion 72.8% (2% above baseline, slightly outperformed by SVM</li>
<li>Rule- and lexicon-based algorithm (not ML)</li>
<li>"Gold standard", i.e. test labels: mean of sentiment scores as assigned by three female operators</li>
</ul>

<p>From SentiStrength follow-up (Someone et al., resource where ???):</p>

<ul>
<li>SentiStrength is outperformed on Twitter data by "Best ML algorithm" for positive (60.7% vs. 56.3%) and negative (64.3% vs. 61.7%) emotion accuracy and also has lower correlations with human-coded scores (~2% difference)</li>
</ul>

<p>From DAN2 paper (Someone et al., 2016, resource where ???):</p>

<ul>
<li>DAN2 algorithm based on Dynamic ANNs with feature engineering achieves 85.5% accuracy for a 5-class sentiment classification on Twitter data, outperforming SVMs (the standard industry ML algo for this) by 7%.</li>
</ul>

<p>From SentiBench/Ribero paper <a href="https://github.com/SamuelsGitHub/Social_Network_Project/blob/main/paper_sentibench.pdf">SentiBench paper (Ribeiro et al., 2016)</a>:</p>

<ul>
<li>Paper was referenced by Jonas as justification for using SentiStrength</li>
<li>For a 3-class sentiment classification on the Twitter part from a "benchmark" data set, SentiStrength is clearly outperformed by VADER; Macro-F1 46.77% vs. 58.12%, Acc 57.83% vs. 60.21%</li>
<li>For 2-class sentiment classification on the same data, SentiStrength shows terrible coverage in comparison to VADER, i.e. most tweets are not classified; Coverage 34.65% vs. 94.4%, Acc 96.97% vs. 99.04%;</li>
<li>For general sentiment analysis methods on 3 classes, VADER ranks #1 and SentiStrength ranks #15</li>
<li>For social network-specific sentiment analysis on 3 classes, VADER ranks #3 and SentiStrength ranks #11</li>
<li>From the paper: "Although SentiStrength presented good Macro-F1 values, its coverage is usually low as this method tends to classify a high number of instances as neutral". Thus when Jonas filters out low valences, he indeed removes drastically large amounts of data.</li>
<li>Jonas even used and compared VADER to SentiStrength, and even though they (only) have a correlation of 52-62% in the scoring, he decided to stick to SentiStrength. Not sure why, given the above arguments.</li>
<li>Jonas mentions importance of accurate sentiment analysis tool in "Limitations", but then quotes wrong number about how good SentiStrength is referring to the SentiBench/Ribeiro paper.</li>
</ul>

<p><strong>Conclusions:</strong></p>

<ul>
<li>Considering Jonas’ research is from 2021, he could have used more recent and powerful approaches to obtain sentiment intensity scores</li>
<li>Sentiment analysis libraries for Python: NLTK, SpaCy, TextBlob, CoreNLP, gensim (topic analysis), Flair</li>
</ul>

<h3>3. Preprocessing + valence analysis</h3>

<ul>
<li>SentiStrength returns scores in [1,5] for positive emotion and [-5,-1] for negative emotion. Jonas decided to scale both to [0, 4], i.e. subtract 1 and take absolute values for the retweet predictions.</li>
<li>His argument: easier to interpret regression model intercepts. But then again he uses retweets_reciprocal which makes direct interpretation of coefficients virtually impossible.</li>
<li><p>Does this scaling change the effect of the features?</p></li>
<li><p>Valence analysis is essentially comparing the means of sen<em>pos and sen</em>neg and quantifying a significant difference. The important part in the LMER output are the coefficients and specifically their signs. Intercept = mean of sen_neg.</p></li>
<li>Why not simply do a standard paired t.test instead? <code>t.test(x=df$sen_neg, y=df$sen_pos, alternative="greater", paired=TRUE)</code> gives the desired result and is much more straightforward.</li>
<li><code>t.test</code> yields different result for <code>d.trump</code>?</li>
</ul>

<h3>4. Retweet prediction</h3>

<ul>
<li>Why is user’s #followers included, if Jonas explicitly mentions "users with more followers generally have more retweets regardless of the emotional content of their tweets"</li>
<li>The variable has (relatively) high explanatory power and contains most of the explained variance of his final fitted models (see e.g. drop1 RSS)</li>
<li>Is it included just to bump up the R^2 values to make it more believable?</li>
<li>Features sen<em>pos and sen</em>neg were not encoded as factors?</li>
</ul>

<h4>Models and transformations tested by Jonas:</h4>

<ul>
<li><code>lmer + retweets_reciprocal</code> (FINAL MODEL)</li>
<li><code>lmer + retweets_log</code></li>
<li><code>glmer + family=poisson(link="log") + retweets</code></li>
<li><code>glmer.nb (neg binomial) + retweets</code></li>
<li><code>GAMM + family=gaussian + retweets_reciprocal + s(sen_pos/neg, k=4)</code></li>
</ul>

<h4>Potential models/transformations (relating to skewed data):</h4>

<ul>
<li>Simple normalization of data</li>
<li>Square root transformation</li>
<li>Box-Cox transformation</li>
<li>Transformation of covariates (see point below)</li>
<li>Multinomial/categorical distribution</li>
<li>Pareto distribution</li>
<li>Ordinal regression (e.g. Cumulative Link Mixed Model CLMM)</li>
<li>Non-parametric/non-linear models (Comp Stats-style)</li>
</ul>

<p>Personal opinion:
- Seems to me Jonas decided to use Linear Mixed Models without questioning
- Potentially misspecified model and faulty p-value interpretation
- TA plot looks weird (see reference: https://stats.stackexchange.com/questions/260081/residual-pattern-in-mixed-model) -> Covariate transform?
- QQ plot looks like systematic deviation
- Are Linear Mixed Model assumptions violated ?
- The fact that the target variable has to be so strongly transformed to make it work may hint that the true relationship is in fact non-linear.</p>

<h4>Incorrect backtransform (MISTAKE)</h4>

<ul>
<li>Initially, the model outputs for retweet prediction were not accurately reproducible, values being slightly off. Then I found out why.</li>
<li>Jonas did a mistake when computing retweets from the backtransform of <code>retweets_log</code>: he used <code>exp(retweets_log-1)</code> instead of <code>exp(retweets_log)-1</code></li>
<li>when using <code>retweets = round(exp(retweets_log - 1))</code> and running the models with <code>retweets_reciprocal</code> based on these incorrect values, I obtain his results.</li>
<li>Not a huge difference in the final interpretation/significance, but coefficient values etc. definitely change</li>
<li>Since he made this mistake, the estimated effects used for the plots are also wrong.</li>
</ul>

<h3>5. Political affiliation</h3>

<p>Comments are mainly based on <code>d.trump</code>.</p>

<ul>
<li>The random sample of 30830 tweets represents only 11% of d.trump, it could be an unfortunate sample, it is not evident to generalise the observed affiliation distribution to whole dataset.</li>
<li><p>For <code>d.samesex</code>, the sample is 4816 tweets and is supposedly generalisable to dataset of size 980 018.</p></li>
<li><p>Jonas writes "able to estimate affiliation for 85.9% users tested" and "tested 30830 tweets"; but then goes on to fully separate ALL the 30830 tweets into Dem/Rep, not 85.9% of them?</p></li>
<li><p>Binary assignment to Rep/Dem was made purely based on <code>max(#Dem follows, #Rep follows)</code>, and if equal then Dem ("Mid" labels).</p></li>
<li>This is a very naive way of assigning affiliation: consider someone following 51 Rep and 50 Dem from the list -> Rep; consider someone following 1000 people of which 1 is Dem and 2 are Rep on the list -> Rep;</li>
<li><p>More sensible to work with thresholds, e.g. min. ratio of recognized accounts/total followed accounts, and min. difference ratio between Dem and Rep accounts to state a valid affiliation.</p></li>
<li><p>List of accounts labelled Dem/Rep from Bail et al. is unbalanced (more Dems) and may not be extensive enough to properly base conclusions</p></li>
<li>Furthermore, their public figure list attributes accounts a liberal/conservative score on a spectrum, NOT a partisan Dem/Rep label.</li>
<li>Thus when saying that most tweets were made by "conservatives", this does not exclude Dem conservatives and vice versa.</li>
<li>Claims for intragroup vs. intergroup effects cannot be made because we cannot properly disentangle political views from party membership.</li>
<li><p>Jonas mentions the fact that intragroup interactions may have not been properly captured in "Limitations" - I agree.</p></li>
<li><p>When filtering data for the stated subset (30830 tweets) with correct/incorrect retweet values I only obtain 11932 tweets. </p></li>
<li><p>When filtering for "Rep" one can see that the stated conditions (min. 1 retweet, min 2 intensity score) are not fulfilled, explaining why the subsample is larger than when I filter precisely. Why?</p></li>
<li><p>Rerunning the linear model on the "Rep" subset isn’t evidence.</p></li>
<li>Jonas writes "repeated analysis on the subsample of users identified as conservatives … analysis led to same results as more inclusive analysis". </li>
<li>This is not true? Subsample analysis returns everything significant (incl. interaction and sen_pos) -> can one even evaluate main effects then?</li>
</ul>

<p>—> Using the above points, it is a far reach to claim having intragroup effects. No additional statistical test for this were done.</p>

<h3>6. Text analysis (content of tweets)</h3>

<p>Comments mainly based on d.trump.</p>

<ul>
<li>Issues with assignment of data to election victory/defeat based on very limited amount of hashtags</li>
<li><p>E.g. multiple negative tweets (order by valence) from Hillary supporters using #MAGA or #PresidentTrump, i.e. again strong entanglement of intergroup/intragroup effects.</p></li>
<li><p>One can use examples from d.samesex.text and d.trump.text to show that SentiStrength is doing a somewhat poor job. E.g. look at content of samesex tweets ranked as both sen<em>pos=5 and sen</em>neg=-5.</p></li>
<li>These tweets then get a valence (sum of both) of 0, classifying them as "neutral" even though they clearly are NOT emotionally neutral.</li>
<li><p>Jonas’ valence approach is a very naive way of assigning a binary label, it relies too much on the inaccurate scoring of SentiStrength.</p></li>
<li><p>By later filtering out tweets classified as "neutral" he essentially discards a big chunk of data that is very expressive i.e. has high emotional intensity both ways.</p></li>
<li>Instead he only selects those that go either way, but as we can see from examples these are somewhat arbitrary and not well classified by SentiStrength.</li>
<li><p>Filtering out "neutral" for samesex unigrams decreases data from 446133 to 148007 samples (loses over 2/3 of data!)</p></li>
<li><p>When you look at samesex tweets with valence=-4 (thus classified as very negative) it is many people being super happy and "crying tears of joy … LoveWins".</p></li>
<li>When you look at "negative" top viral tweets for samesex, most of them are super positive.</li>
<li><p>When looking at ap<em>top</em>terms for samesex bigrams for the two topics, they are mostly all positive -> more like one topic.</p></li>
<li><p>Binarization of data into pos/neg doesn’t make use of [1,5] scores</p></li>
<li>Maybe Jonas should have used a sentiment analysis tool that does a simpler binary classification with higher accuracy rather than the granular classification and then aggregating these values which can produce inconsistent assignments.</li>
<li><p>Loss of much info by going from: 
2 factors, 10 lvls (sen<em>neg, sen</em>pos) -> 1 factor, 9 lvls (valence in [-4,4])
-> 1 factor, 3 lvls (pos, neg, neutral) -> 1 factor, 2 lvls (filter neutral)</p></li>
<li><p>Is looking at the most unique unigrams/bigrams really representative of the topic content? </p></li>
<li>Isn’t that kind of cherrypicking, where you select the most extreme and potentially rare cases? Shouldn’t one just look at the most frequent grams/highest beta score?</li>
<li>In that case, both topics would be revealed to pretty much be intermixed, without a clear distinction and with much overlap of grams, even though the gamma values supposedly claim a clear distinction.</li>
<li>High gamma values claiming "means that words appearing in a category have higher prob to only be used in one of the topics" are disproved by the overlap of ap<em>top</em>terms?</li>
<li>There is something strange going on with the LDA and only having 2 topics when giving it data with a binary labeled topic column... I get the feeling that the results are somewhat predetermined by the inputs and topic size.</li>
<li>There is very high accordance of ap<em>top</em>terms with the gay_dtm values with highest counts, i.e. words that have high counts are also identified as words representing the topic (makes sense) 
-> Again, unique words are cherrypicking? Or is this common practice in topic modelling?</li>
</ul>
